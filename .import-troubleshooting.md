# NexusCodex Import Script - Troubleshooting Documentation

This document chronicles all issues encountered and fixes applied while implementing the D&D rulebook import functionality.

## Overview

**Goal**: Import 193 D&D PDF rulebooks from `/Users/JoelN/Documents/DnD/5e Rulebooks` into NexusCodex permanently, preserving folder structure as tags.

**Final Status**: ‚úÖ Import script working successfully. Test import of 3 core rulebooks completed.

---

## Issue 1: Duplicate Route Definition Causing Fastify Crash

### Problem
```
FastifyError: Method 'GET' already declared for route '/api/search/quick'
```

The doc-api container was crashing on startup because the `/api/search/quick` route was defined in both `search.ts` and `structured-data.ts`.

### Root Cause
During development, the same endpoint was accidentally implemented in two different route files:
- `services/doc-api/src/routes/search.ts`
- `services/doc-api/src/routes/structured-data.ts`

### Solution
**File**: `services/doc-api/src/routes/structured-data.ts`

Removed the duplicate route definition (lines 131-200) and the unused import:
```typescript
// REMOVED: import { QuickSearchQuery } from '../types/structured-data';
```

The canonical implementation remains in `search.ts`.

**Commit Reference**: Fixed duplicate route definition in structured-data.ts

---

## Issue 2: doc-websocket Health Check Failure

### Problem
```
‚úó doc-websocket health check failed
```

The `test-stack.sh` script was failing because doc-websocket's health endpoint returned `status: 'ok'` but the test script expected `status: 'healthy'`.

### Root Cause
Inconsistency between services:
- `doc-api` returned: `{"status": "healthy", ...}`
- `doc-websocket` returned: `{"status": "ok", ...}`

The test script grepped for "healthy" which failed for doc-websocket.

### Solution
**File**: `services/doc-websocket/src/index.ts:18`

Changed the health endpoint response to match doc-api format:
```typescript
app.get('/health', (_req, res) => {
  res.json({
    status: 'healthy',  // Changed from 'ok'
    service: 'doc-websocket',
    timestamp: new Date().toISOString(),
  });
});
```

**Rebuild**: `docker compose build doc-websocket && docker compose up -d doc-websocket`

---

## Issue 3: MinIO Health Check Failure

### Problem
```
‚úó MinIO health check failed
```

The test script was checking for the string "OK" in MinIO's health response, but MinIO returns an empty body with HTTP 200.

### Root Cause
MinIO's `/minio/health/live` endpoint returns:
```
HTTP/1.1 200 OK
Content-Length: 0
```

The test was using: `curl -s http://localhost:9000/minio/health/live | grep -q "OK"`

### Solution
**File**: `test-stack.sh:58`

Changed to check HTTP status code instead of response body:
```bash
# Before
if curl -s http://localhost:9000/minio/health/live | grep -q "OK"; then

# After
if curl -sf http://localhost:9000/minio/health/live > /dev/null 2>&1; then
```

The `-f` flag makes curl fail on HTTP errors, and we redirect output since we only care about the exit code.

---

## Issue 4: Import Script - Globstar Shell Option Error

### Problem
```
./scripts/import-dnd-rulebooks.sh: line 299: shopt: globstar: invalid shell option name
```

The script used bash-specific `globstar` feature which isn't available in all shells.

### Root Cause
Original code:
```bash
shopt -s globstar nullglob
PDF_FILES=("$SOURCE_DIR"/**/*.pdf)
```

The `globstar` option is bash 4+ specific and not available in sh or older bash versions.

### Solution
**File**: `scripts/import-dnd-rulebooks.sh:297-314`

Replaced with portable `find` command:
```bash
# Use find to locate all PDF files (more portable than globstar)
PDF_FILES=()
while IFS= read -r -d '' file; do
  PDF_FILES+=("$file")
done < <(find "$SOURCE_DIR" -type f -name "*.pdf" -print0)

TOTAL_FILES=${#PDF_FILES[@]}
```

This approach:
- Works in any POSIX-compliant shell
- Handles filenames with spaces correctly (null-delimited with `-print0` and `-d ''`)
- Recursively finds all PDFs

---

## Issue 5: Document ID Not Returned from API

### Problem
```
‚úó Error: No document ID returned
```

The import script couldn't extract the document ID from the API response.

### Root Cause
API returns nested structure:
```json
{
  "document": {
    "id": "abc-123",
    "title": "..."
  },
  "uploadUrl": "..."
}
```

But script was trying to extract `.id` directly instead of `.document.id`.

### Solution
**File**: `scripts/import-dnd-rulebooks.sh:275`

Fixed the jq query:
```bash
# Before
local doc_id=$(echo "$create_response" | jq -r '.id')

# After
local doc_id=$(echo "$create_response" | jq -r '.document.id')
```

Also added debug output to help troubleshoot API responses:
```bash
if [[ "${VERBOSE:-false}" == "true" ]]; then
  echo -e "  DEBUG: API Response:"
  echo "$create_response" | jq .
fi
```

---

## Issue 6: S3 Pre-Signed URL Host Mismatch (HTTP 403)

### Problem
```
‚úó Error uploading file (HTTP 403)
```

File uploads were failing even though the upload URL was correct. Initial attempts showed:
- HTTP 000: Connection failed (URL malformed)
- HTTP 403: Forbidden (signature invalid)

### Root Cause - Part A: URL Replacement Bug

First issue was the bash string replacement leaving escaped slashes:
```bash
upload_url="${upload_url//http:\/\/minio:9000/http:\/\/localhost:9000}"
```

This produced: `http:\/\/localhost:9000` instead of `http://localhost:9000`

**Solution**: Simplified replacement to just the hostname:
```bash
upload_url="${upload_url//minio:9000/localhost:9000}"
```

### Root Cause - Part B: AWS Signature Validation

Even after fixing the URL format, HTTP 403 persisted. The fundamental issue:

1. S3 service inside Docker generated pre-signed URL with endpoint `http://minio:9000`
2. AWS signature includes the Host header in its calculation
3. Script replaced `minio:9000` with `localhost:9000` in the URL
4. When curl sent the request to `localhost:9000`, the Host header didn't match the signature
5. MinIO rejected the request as having an invalid signature

### Solution: Separate S3 Client for Public URLs

Created a two-client architecture where pre-signed URLs are generated with the public endpoint from the start.

**Step 1**: Add environment variable support

**File**: `services/doc-api/src/config/env.ts:23-30`
```typescript
// S3/MinIO
S3_ENDPOINT: z.string(),
S3_PUBLIC_ENDPOINT: z.string().optional(), // Public endpoint for pre-signed URLs
S3_ACCESS_KEY: z.string(),
S3_SECRET_KEY: z.string(),
S3_BUCKET: z.string().default('documents'),
S3_REGION: z.string().default('us-east-1'),
S3_FORCE_PATH_STYLE: z.string().default('true').transform(val => val === 'true'),
```

**Step 2**: Configure docker-compose

**File**: `docker-compose.yml:11-12`
```yaml
environment:
  - S3_ENDPOINT=http://minio:9000          # Internal Docker network
  - S3_PUBLIC_ENDPOINT=http://localhost:9000  # External host access
  - S3_ACCESS_KEY=admin
  - S3_SECRET_KEY=password
```

**Step 3**: Create separate S3 clients

**File**: `services/doc-api/src/services/s3.service.ts:12-41`
```typescript
class S3Service {
  private client: S3Client;        // For internal operations
  private publicClient: S3Client;  // For public pre-signed URLs
  private bucket: string;

  constructor() {
    // Internal client for Docker network operations
    this.client = new S3Client({
      endpoint: env.S3_ENDPOINT,  // http://minio:9000
      region: env.S3_REGION,
      credentials: {
        accessKeyId: env.S3_ACCESS_KEY,
        secretAccessKey: env.S3_SECRET_KEY,
      },
      forcePathStyle: env.S3_FORCE_PATH_STYLE,
    });

    // Public client for generating URLs accessible from host machine
    const publicEndpoint = env.S3_PUBLIC_ENDPOINT || env.S3_ENDPOINT;
    this.publicClient = new S3Client({
      endpoint: publicEndpoint,  // http://localhost:9000
      region: env.S3_REGION,
      credentials: {
        accessKeyId: env.S3_ACCESS_KEY,
        secretAccessKey: env.S3_SECRET_KEY,
      },
      forcePathStyle: env.S3_FORCE_PATH_STYLE,
    });

    this.bucket = env.S3_BUCKET;
  }
}
```

**Step 4**: Use public client for pre-signed URLs

**File**: `services/doc-api/src/services/s3.service.ts:64-75`
```typescript
async getUploadUrl(key: string, contentType: string): Promise<string> {
  const command = new PutObjectCommand({
    Bucket: this.bucket,
    Key: key,
    ContentType: contentType,
  });

  // Use publicClient so URLs work from outside Docker
  return getSignedUrl(this.publicClient, command, {
    expiresIn: env.UPLOAD_URL_EXPIRY,
  });
}
```

**Step 5**: Rebuild and restart
```bash
docker compose build doc-api && docker compose up -d doc-api
```

### Why This Solution Works

1. The AWS SDK generates the signature using the endpoint configured in the S3Client
2. `publicClient` is configured with `http://localhost:9000` from the start
3. The signature is computed with `localhost:9000` as the Host
4. When curl sends the request to `localhost:9000`, the Host header matches
5. MinIO validates the signature successfully

### Alternative Approaches Considered

**‚ùå Attempted**: Running curl inside Docker container
- Issue: Alpine container didn't have curl installed
- Would require modifying Dockerfile or using docker exec with file copying

**‚ùå Attempted**: Post-generation URL manipulation
- Issue: AWS signatures include the Host header, so changing the host invalidates the signature
- This is why simple string replacement failed

**‚úÖ Chosen**: Dual-client architecture
- Clean separation of concerns
- No signature manipulation
- Works for both local development and production
- Easy to configure via environment variables

---

## Issue 7: Import Script - curl via Docker Exec Failed

### Problem
```
sh: curl: not found
```

Attempted workaround of running curl inside the doc-api container failed because Alpine Linux doesn't include curl by default.

### Context
This was tried as a workaround for Issue #6 before implementing the proper dual-client solution.

```bash
# This failed
local upload_status=$(docker compose exec -T doc-api sh -c "curl -s -X PUT '$upload_url' \
  -H 'Content-Type: application/pdf' \
  --data-binary @- \
  -o /dev/null -w '%{http_code}'" < "$file_path")
```

### Resolution
Not fixed directly - instead solved by implementing S3_PUBLIC_ENDPOINT (Issue #6). The script now uses curl from the host machine with properly signed URLs.

---

## Final Working Architecture

### File Upload Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Import Script  ‚îÇ (Host machine)
‚îÇ  (Bash/curl)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ 1. POST /api/documents
         ‚îÇ    {title, type, fileSize, ...}
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  doc-api (Docker)       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  publicClient    ‚îÇ   ‚îÇ ‚Üê Configured with http://localhost:9000
‚îÇ  ‚îÇ  (S3Client)      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ
‚îÇ         ‚îÇ 2. Generate    ‚îÇ
‚îÇ         ‚îÇ    pre-signed  ‚îÇ
‚îÇ         ‚îÇ    URL         ‚îÇ
‚îÇ         ‚ñº                ‚îÇ
‚îÇ  http://localhost:9000/  ‚îÇ
‚îÇ  documents/xyz.pdf?      ‚îÇ
‚îÇ  X-Amz-Signature=...     ‚îÇ ‚Üê Signature computed with localhost:9000
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ 3. Return URL to script
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Import Script   ‚îÇ
‚îÇ  curl PUT        ‚îÇ ‚Üê Uploads to localhost:9000
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ 4. Upload file
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MinIO (Docker)      ‚îÇ ‚Üê Listens on 0.0.0.0:9000
‚îÇ  Validates signature ‚îÇ ‚Üê localhost:9000 matches signature
‚îÇ  Stores file         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Configuration Files

**docker-compose.yml**:
```yaml
services:
  doc-api:
    environment:
      - S3_ENDPOINT=http://minio:9000          # Internal
      - S3_PUBLIC_ENDPOINT=http://localhost:9000  # External
```

**services/doc-api/src/config/env.ts**:
```typescript
S3_ENDPOINT: z.string(),
S3_PUBLIC_ENDPOINT: z.string().optional(),
```

**services/doc-api/src/services/s3.service.ts**:
- `this.client` - Uses S3_ENDPOINT (internal)
- `this.publicClient` - Uses S3_PUBLIC_ENDPOINT (external)
- `getUploadUrl()` - Uses publicClient

---

## Successful Test Results

### Import Summary
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    Import Summary                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

  Total Files:     3
  Imported:        3
  Skipped:         0
  Errors:          0

Import completed successfully!
```

### Documents Imported
1. **Player's Handbook** (80MiB) - `d8122a93-71fc-4683-bd23-aec406d87605`
2. **Monster Manual** (28MiB) - `21692615-3941-4a93-9897-295a782f360d`
3. **Dungeon Master's Guide** (88MiB) - `adc62b3b-42d2-4e9b-a1d3-7638d33a1146`

All files uploaded successfully with:
- Correct folder tags (core)
- Proper document types (rulebook)
- Processing jobs triggered

### API Verification
```bash
curl -s http://localhost:3000/api/documents | jq '.documents[] | {title, type, tags}'
```

Returns all three documents with proper metadata.

---

## Known Issues (Non-Critical)

### Thumbnail Generation Error
```
Failed to generate thumbnail: Please provide binary data as `Uint8Array`, rather than `Buffer`.
```

**Status**: Does not affect import functionality. This is a separate processing bug in the doc-processor service related to Buffer/Uint8Array type handling in the thumbnail generation code.

**Impact**: Documents are imported and stored successfully. Text extraction works. Only thumbnail generation fails.

**Location**: `services/doc-processor/src/services/thumbnail.service.ts`

**Next Steps**: Needs investigation and fix in the thumbnail service, but doesn't block the import functionality.

---

## Commands Reference

### Test Stack Health
```bash
./test-stack.sh
```

### Import with Dry Run
```bash
./scripts/import-dnd-rulebooks.sh --dry-run
```

### Import Specific Directory
```bash
./scripts/import-dnd-rulebooks.sh --source-dir /path/to/pdfs
```

### Import with Verbose Debug Output
```bash
VERBOSE=true ./scripts/import-dnd-rulebooks.sh --source-dir /path/to/pdfs
```

### Check Import Status
```bash
# View all documents
curl -s http://localhost:3000/api/documents | jq '.documents[] | {title, type, tags}'

# Check processing logs
docker compose logs -f doc-processor

# View MinIO files
open http://localhost:9001  # admin/password
```

### Rebuild Services After Code Changes
```bash
# Rebuild specific service
docker compose build doc-api
docker compose up -d doc-api

# Rebuild all services
docker compose build
docker compose up -d
```

---

## Lessons Learned

1. **AWS Signatures are Strict**: Cannot manipulate URLs after signature generation without invalidating them. The Host header is part of the signature calculation.

2. **Docker Networking**: Services inside Docker use internal hostnames (minio:9000), but external access requires localhost:9000. Need different endpoints for internal vs external access.

3. **Shell Portability**: Avoid bash-specific features like `globstar`. Use POSIX-compliant alternatives like `find`.

4. **API Response Structure**: Always check the actual API response structure. Nested objects require adjusted jq queries.

5. **Health Check Consistency**: All services should return the same health check format for easier testing.

6. **Debug Output**: Adding verbose mode and debug logging early saves significant troubleshooting time.

7. **Test with Small Datasets First**: Testing with 3 files instead of 193 made debugging much faster and cheaper in terms of storage/processing.

---

## Production Deployment Notes

When deploying to production (GCP):

1. **S3_PUBLIC_ENDPOINT**: Set to your GCS public URL or load balancer URL
2. **Pre-signed URL Expiry**: Consider reducing `UPLOAD_URL_EXPIRY` from 3600s (1 hour)
3. **CORS Configuration**: Ensure GCS bucket has proper CORS rules if uploading from browser
4. **Authentication**: Add authentication to the import script for production use
5. **Rate Limiting**: Consider adding rate limiting to the document creation endpoint
6. **Monitoring**: Set up alerts for failed imports and processing jobs

---

## Timeline Summary

1. ‚úÖ Created import script with folder-to-tag mapping
2. ‚úÖ Fixed duplicate route causing container crash
3. ‚úÖ Fixed doc-websocket health check format mismatch
4. ‚úÖ Fixed MinIO health check in test script
5. ‚úÖ Fixed globstar compatibility issue
6. ‚úÖ Fixed document ID extraction from API response
7. ‚úÖ Fixed S3 pre-signed URL signature validation with dual-client architecture
8. ‚úÖ Successfully imported 3 test documents
9. üìã Ready for full 193-document import

**Total Issues Resolved**: 7 major issues
**Final Status**: ‚úÖ All systems operational, import script fully functional
