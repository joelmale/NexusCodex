# NexusCodex Import Script - Troubleshooting Documentation

This document chronicles all issues encountered and fixes applied while implementing the D&D rulebook import functionality.

## Overview

**Goal**: Import 193 D&D PDF rulebooks from `/Users/JoelN/Documents/DnD/5e Rulebooks` into NexusCodex permanently, preserving folder structure as tags.

**Final Status**: ✅ Import script working successfully. Test import of 3 core rulebooks completed.

---

## Issue 1: Duplicate Route Definition Causing Fastify Crash

### Problem
```
FastifyError: Method 'GET' already declared for route '/api/search/quick'
```

The doc-api container was crashing on startup because the `/api/search/quick` route was defined in both `search.ts` and `structured-data.ts`.

### Root Cause
During development, the same endpoint was accidentally implemented in two different route files:
- `services/doc-api/src/routes/search.ts`
- `services/doc-api/src/routes/structured-data.ts`

### Solution
**File**: `services/doc-api/src/routes/structured-data.ts`

Removed the duplicate route definition (lines 131-200) and the unused import:
```typescript
// REMOVED: import { QuickSearchQuery } from '../types/structured-data';
```

The canonical implementation remains in `search.ts`.

**Commit Reference**: Fixed duplicate route definition in structured-data.ts

---

## Issue 2: doc-websocket Health Check Failure

### Problem
```
✗ doc-websocket health check failed
```

The `test-stack.sh` script was failing because doc-websocket's health endpoint returned `status: 'ok'` but the test script expected `status: 'healthy'`.

### Root Cause
Inconsistency between services:
- `doc-api` returned: `{"status": "healthy", ...}`
- `doc-websocket` returned: `{"status": "ok", ...}`

The test script grepped for "healthy" which failed for doc-websocket.

### Solution
**File**: `services/doc-websocket/src/index.ts:18`

Changed the health endpoint response to match doc-api format:
```typescript
app.get('/health', (_req, res) => {
  res.json({
    status: 'healthy',  // Changed from 'ok'
    service: 'doc-websocket',
    timestamp: new Date().toISOString(),
  });
});
```

**Rebuild**: `docker compose build doc-websocket && docker compose up -d doc-websocket`

---

## Issue 3: MinIO Health Check Failure

### Problem
```
✗ MinIO health check failed
```

The test script was checking for the string "OK" in MinIO's health response, but MinIO returns an empty body with HTTP 200.

### Root Cause
MinIO's `/minio/health/live` endpoint returns:
```
HTTP/1.1 200 OK
Content-Length: 0
```

The test was using: `curl -s http://localhost:9000/minio/health/live | grep -q "OK"`

### Solution
**File**: `test-stack.sh:58`

Changed to check HTTP status code instead of response body:
```bash
# Before
if curl -s http://localhost:9000/minio/health/live | grep -q "OK"; then

# After
if curl -sf http://localhost:9000/minio/health/live > /dev/null 2>&1; then
```

The `-f` flag makes curl fail on HTTP errors, and we redirect output since we only care about the exit code.

---

## Issue 4: Import Script - Globstar Shell Option Error

### Problem
```
./scripts/import-dnd-rulebooks.sh: line 299: shopt: globstar: invalid shell option name
```

The script used bash-specific `globstar` feature which isn't available in all shells.

### Root Cause
Original code:
```bash
shopt -s globstar nullglob
PDF_FILES=("$SOURCE_DIR"/**/*.pdf)
```

The `globstar` option is bash 4+ specific and not available in sh or older bash versions.

### Solution
**File**: `scripts/import-dnd-rulebooks.sh:297-314`

Replaced with portable `find` command:
```bash
# Use find to locate all PDF files (more portable than globstar)
PDF_FILES=()
while IFS= read -r -d '' file; do
  PDF_FILES+=("$file")
done < <(find "$SOURCE_DIR" -type f -name "*.pdf" -print0)

TOTAL_FILES=${#PDF_FILES[@]}
```

This approach:
- Works in any POSIX-compliant shell
- Handles filenames with spaces correctly (null-delimited with `-print0` and `-d ''`)
- Recursively finds all PDFs

---

## Issue 5: Document ID Not Returned from API

### Problem
```
✗ Error: No document ID returned
```

The import script couldn't extract the document ID from the API response.

### Root Cause
API returns nested structure:
```json
{
  "document": {
    "id": "abc-123",
    "title": "..."
  },
  "uploadUrl": "..."
}
```

But script was trying to extract `.id` directly instead of `.document.id`.

### Solution
**File**: `scripts/import-dnd-rulebooks.sh:275`

Fixed the jq query:
```bash
# Before
local doc_id=$(echo "$create_response" | jq -r '.id')

# After
local doc_id=$(echo "$create_response" | jq -r '.document.id')
```

Also added debug output to help troubleshoot API responses:
```bash
if [[ "${VERBOSE:-false}" == "true" ]]; then
  echo -e "  DEBUG: API Response:"
  echo "$create_response" | jq .
fi
```

---

## Issue 6: S3 Pre-Signed URL Host Mismatch (HTTP 403)

### Problem
```
✗ Error uploading file (HTTP 403)
```

File uploads were failing even though the upload URL was correct. Initial attempts showed:
- HTTP 000: Connection failed (URL malformed)
- HTTP 403: Forbidden (signature invalid)

### Root Cause - Part A: URL Replacement Bug

First issue was the bash string replacement leaving escaped slashes:
```bash
upload_url="${upload_url//http:\/\/minio:9000/http:\/\/localhost:9000}"
```

This produced: `http:\/\/localhost:9000` instead of `http://localhost:9000`

**Solution**: Simplified replacement to just the hostname:
```bash
upload_url="${upload_url//minio:9000/localhost:9000}"
```

### Root Cause - Part B: AWS Signature Validation

Even after fixing the URL format, HTTP 403 persisted. The fundamental issue:

1. S3 service inside Docker generated pre-signed URL with endpoint `http://minio:9000`
2. AWS signature includes the Host header in its calculation
3. Script replaced `minio:9000` with `localhost:9000` in the URL
4. When curl sent the request to `localhost:9000`, the Host header didn't match the signature
5. MinIO rejected the request as having an invalid signature

### Solution: Separate S3 Client for Public URLs

Created a two-client architecture where pre-signed URLs are generated with the public endpoint from the start.

**Step 1**: Add environment variable support

**File**: `services/doc-api/src/config/env.ts:23-30`
```typescript
// S3/MinIO
S3_ENDPOINT: z.string(),
S3_PUBLIC_ENDPOINT: z.string().optional(), // Public endpoint for pre-signed URLs
S3_ACCESS_KEY: z.string(),
S3_SECRET_KEY: z.string(),
S3_BUCKET: z.string().default('documents'),
S3_REGION: z.string().default('us-east-1'),
S3_FORCE_PATH_STYLE: z.string().default('true').transform(val => val === 'true'),
```

**Step 2**: Configure docker-compose

**File**: `docker-compose.yml:11-12`
```yaml
environment:
  - S3_ENDPOINT=http://minio:9000          # Internal Docker network
  - S3_PUBLIC_ENDPOINT=http://localhost:9000  # External host access
  - S3_ACCESS_KEY=admin
  - S3_SECRET_KEY=password
```

**Step 3**: Create separate S3 clients

**File**: `services/doc-api/src/services/s3.service.ts:12-41`
```typescript
class S3Service {
  private client: S3Client;        // For internal operations
  private publicClient: S3Client;  // For public pre-signed URLs
  private bucket: string;

  constructor() {
    // Internal client for Docker network operations
    this.client = new S3Client({
      endpoint: env.S3_ENDPOINT,  // http://minio:9000
      region: env.S3_REGION,
      credentials: {
        accessKeyId: env.S3_ACCESS_KEY,
        secretAccessKey: env.S3_SECRET_KEY,
      },
      forcePathStyle: env.S3_FORCE_PATH_STYLE,
    });

    // Public client for generating URLs accessible from host machine
    const publicEndpoint = env.S3_PUBLIC_ENDPOINT || env.S3_ENDPOINT;
    this.publicClient = new S3Client({
      endpoint: publicEndpoint,  // http://localhost:9000
      region: env.S3_REGION,
      credentials: {
        accessKeyId: env.S3_ACCESS_KEY,
        secretAccessKey: env.S3_SECRET_KEY,
      },
      forcePathStyle: env.S3_FORCE_PATH_STYLE,
    });

    this.bucket = env.S3_BUCKET;
  }
}
```

**Step 4**: Use public client for pre-signed URLs

**File**: `services/doc-api/src/services/s3.service.ts:64-75`
```typescript
async getUploadUrl(key: string, contentType: string): Promise<string> {
  const command = new PutObjectCommand({
    Bucket: this.bucket,
    Key: key,
    ContentType: contentType,
  });

  // Use publicClient so URLs work from outside Docker
  return getSignedUrl(this.publicClient, command, {
    expiresIn: env.UPLOAD_URL_EXPIRY,
  });
}
```

**Step 5**: Rebuild and restart
```bash
docker compose build doc-api && docker compose up -d doc-api
```

### Why This Solution Works

1. The AWS SDK generates the signature using the endpoint configured in the S3Client
2. `publicClient` is configured with `http://localhost:9000` from the start
3. The signature is computed with `localhost:9000` as the Host
4. When curl sends the request to `localhost:9000`, the Host header matches
5. MinIO validates the signature successfully

### Alternative Approaches Considered

**❌ Attempted**: Running curl inside Docker container
- Issue: Alpine container didn't have curl installed
- Would require modifying Dockerfile or using docker exec with file copying

**❌ Attempted**: Post-generation URL manipulation
- Issue: AWS signatures include the Host header, so changing the host invalidates the signature
- This is why simple string replacement failed

**✅ Chosen**: Dual-client architecture
- Clean separation of concerns
- No signature manipulation
- Works for both local development and production
- Easy to configure via environment variables

---

## Issue 7: Import Script - curl via Docker Exec Failed

### Problem
```
sh: curl: not found
```

Attempted workaround of running curl inside the doc-api container failed because Alpine Linux doesn't include curl by default.

### Context
This was tried as a workaround for Issue #6 before implementing the proper dual-client solution.

```bash
# This failed
local upload_status=$(docker compose exec -T doc-api sh -c "curl -s -X PUT '$upload_url' \
  -H 'Content-Type: application/pdf' \
  --data-binary @- \
  -o /dev/null -w '%{http_code}'" < "$file_path")
```

### Resolution
Not fixed directly - instead solved by implementing S3_PUBLIC_ENDPOINT (Issue #6). The script now uses curl from the host machine with properly signed URLs.

---

## Final Working Architecture

### File Upload Flow

```
┌─────────────────┐
│  Import Script  │ (Host machine)
│  (Bash/curl)    │
└────────┬────────┘
         │
         │ 1. POST /api/documents
         │    {title, type, fileSize, ...}
         ▼
┌─────────────────────────┐
│  doc-api (Docker)       │
│  ┌──────────────────┐   │
│  │  publicClient    │   │ ← Configured with http://localhost:9000
│  │  (S3Client)      │   │
│  └──────────────────┘   │
│         │                │
│         │ 2. Generate    │
│         │    pre-signed  │
│         │    URL         │
│         ▼                │
│  http://localhost:9000/  │
│  documents/xyz.pdf?      │
│  X-Amz-Signature=...     │ ← Signature computed with localhost:9000
└─────────┬───────────────┘
          │
          │ 3. Return URL to script
          │
┌─────────▼────────┐
│  Import Script   │
│  curl PUT        │ ← Uploads to localhost:9000
└─────────┬────────┘
          │
          │ 4. Upload file
          │
┌─────────▼────────────┐
│  MinIO (Docker)      │ ← Listens on 0.0.0.0:9000
│  Validates signature │ ← localhost:9000 matches signature
│  Stores file         │
└──────────────────────┘
```

### Key Configuration Files

**docker-compose.yml**:
```yaml
services:
  doc-api:
    environment:
      - S3_ENDPOINT=http://minio:9000          # Internal
      - S3_PUBLIC_ENDPOINT=http://localhost:9000  # External
```

**services/doc-api/src/config/env.ts**:
```typescript
S3_ENDPOINT: z.string(),
S3_PUBLIC_ENDPOINT: z.string().optional(),
```

**services/doc-api/src/services/s3.service.ts**:
- `this.client` - Uses S3_ENDPOINT (internal)
- `this.publicClient` - Uses S3_PUBLIC_ENDPOINT (external)
- `getUploadUrl()` - Uses publicClient

---

## Successful Test Results

### Import Summary
```
╔════════════════════════════════════════════════════════════╗
║                    Import Summary                          ║
╚════════════════════════════════════════════════════════════╝

  Total Files:     3
  Imported:        3
  Skipped:         0
  Errors:          0

Import completed successfully!
```

### Documents Imported
1. **Player's Handbook** (80MiB) - `d8122a93-71fc-4683-bd23-aec406d87605`
2. **Monster Manual** (28MiB) - `21692615-3941-4a93-9897-295a782f360d`
3. **Dungeon Master's Guide** (88MiB) - `adc62b3b-42d2-4e9b-a1d3-7638d33a1146`

All files uploaded successfully with:
- Correct folder tags (core)
- Proper document types (rulebook)
- Processing jobs triggered

### API Verification
```bash
curl -s http://localhost:3000/api/documents | jq '.documents[] | {title, type, tags}'
```

Returns all three documents with proper metadata.

---

## Known Issues (Non-Critical)

### Thumbnail Generation Error
```
Failed to generate thumbnail: Please provide binary data as `Uint8Array`, rather than `Buffer`.
```

**Status**: Does not affect import functionality. This is a separate processing bug in the doc-processor service related to Buffer/Uint8Array type handling in the thumbnail generation code.

**Impact**: Documents are imported and stored successfully. Text extraction works. Only thumbnail generation fails.

**Location**: `services/doc-processor/src/services/thumbnail.service.ts`

**Next Steps**: Needs investigation and fix in the thumbnail service, but doesn't block the import functionality.

---

## Commands Reference

### Test Stack Health
```bash
./test-stack.sh
```

### Import with Dry Run
```bash
./scripts/import-dnd-rulebooks.sh --dry-run
```

### Import Specific Directory
```bash
./scripts/import-dnd-rulebooks.sh --source-dir /path/to/pdfs
```

### Import with Verbose Debug Output
```bash
VERBOSE=true ./scripts/import-dnd-rulebooks.sh --source-dir /path/to/pdfs
```

### Check Import Status
```bash
# View all documents
curl -s http://localhost:3000/api/documents | jq '.documents[] | {title, type, tags}'

# Check processing logs
docker compose logs -f doc-processor

# View MinIO files
open http://localhost:9001  # admin/password
```

### Rebuild Services After Code Changes
```bash
# Rebuild specific service
docker compose build doc-api
docker compose up -d doc-api

# Rebuild all services
docker compose build
docker compose up -d
```

---

## Lessons Learned

1. **AWS Signatures are Strict**: Cannot manipulate URLs after signature generation without invalidating them. The Host header is part of the signature calculation.

2. **Docker Networking**: Services inside Docker use internal hostnames (minio:9000), but external access requires localhost:9000. Need different endpoints for internal vs external access.

3. **Shell Portability**: Avoid bash-specific features like `globstar`. Use POSIX-compliant alternatives like `find`.

4. **API Response Structure**: Always check the actual API response structure. Nested objects require adjusted jq queries.

5. **Health Check Consistency**: All services should return the same health check format for easier testing.

6. **Debug Output**: Adding verbose mode and debug logging early saves significant troubleshooting time.

7. **Test with Small Datasets First**: Testing with 3 files instead of 193 made debugging much faster and cheaper in terms of storage/processing.

---

## Production Deployment Notes

When deploying to production (GCP):

1. **S3_PUBLIC_ENDPOINT**: Set to your GCS public URL or load balancer URL
2. **Pre-signed URL Expiry**: Consider reducing `UPLOAD_URL_EXPIRY` from 3600s (1 hour)
3. **CORS Configuration**: Ensure GCS bucket has proper CORS rules if uploading from browser
4. **Authentication**: Add authentication to the import script for production use
5. **Rate Limiting**: Consider adding rate limiting to the document creation endpoint
6. **Monitoring**: Set up alerts for failed imports and processing jobs

---

## Timeline Summary

1. ✅ Created import script with folder-to-tag mapping
2. ✅ Fixed duplicate route causing container crash
3. ✅ Fixed doc-websocket health check format mismatch
4. ✅ Fixed MinIO health check in test script
5. ✅ Fixed globstar compatibility issue
6. ✅ Fixed document ID extraction from API response
7. ✅ Fixed S3 pre-signed URL signature validation with dual-client architecture
8. ✅ Successfully imported 3 test documents
9. 📋 Ready for full 193-document import

**Total Issues Resolved**: 7 major issues
**Final Status**: ✅ All systems operational, import script fully functional
